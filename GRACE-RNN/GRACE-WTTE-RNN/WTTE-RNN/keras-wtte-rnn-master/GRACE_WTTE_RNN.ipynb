{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine: 0\n",
      "Engine: 1\n",
      "Engine: 2\n",
      "Engine: 3\n",
      "Engine: 4\n",
      "Engine: 5\n",
      "Engine: 6\n",
      "Engine: 7\n",
      "Engine: 8\n",
      "Engine: 9\n",
      "Engine: 10\n",
      "Engine: 11\n",
      "Engine: 12\n",
      "Engine: 13\n",
      "Engine: 14\n",
      "Engine: 15\n",
      "Engine: 16\n",
      "Engine: 17\n",
      "Engine: 18\n",
      "Engine: 19\n",
      "Engine: 20\n",
      "Engine: 21\n",
      "Engine: 22\n",
      "Engine: 23\n",
      "Engine: 24\n",
      "Engine: 25\n",
      "Engine: 26\n",
      "Engine: 27\n",
      "Engine: 28\n",
      "Engine: 29\n",
      "Engine: 30\n",
      "Engine: 31\n",
      "Engine: 32\n",
      "Engine: 33\n",
      "Engine: 34\n",
      "Engine: 35\n",
      "Engine: 36\n",
      "Engine: 37\n",
      "Engine: 38\n",
      "Engine: 39\n",
      "Engine: 40\n",
      "Engine: 41\n",
      "Engine: 42\n",
      "Engine: 43\n",
      "Engine: 44\n",
      "Engine: 45\n",
      "Engine: 46\n",
      "Engine: 47\n",
      "Engine: 48\n",
      "Engine: 49\n",
      "Engine: 50\n",
      "Engine: 51\n",
      "Engine: 52\n",
      "Engine: 53\n",
      "Engine: 54\n",
      "Engine: 55\n",
      "Engine: 56\n",
      "Engine: 57\n",
      "Engine: 58\n",
      "Engine: 59\n",
      "Engine: 60\n",
      "Engine: 61\n",
      "Engine: 62\n",
      "Engine: 63\n",
      "Engine: 64\n",
      "Engine: 65\n",
      "Engine: 66\n",
      "Engine: 67\n",
      "Engine: 68\n",
      "Engine: 69\n",
      "Engine: 70\n",
      "Engine: 71\n",
      "Engine: 72\n",
      "Engine: 73\n",
      "Engine: 74\n",
      "Engine: 75\n",
      "Engine: 76\n",
      "Engine: 77\n",
      "Engine: 78\n",
      "Engine: 79\n",
      "Engine: 80\n",
      "Engine: 81\n",
      "Engine: 82\n",
      "Engine: 83\n",
      "Engine: 84\n",
      "Engine: 85\n",
      "Engine: 86\n",
      "Engine: 87\n",
      "Engine: 88\n",
      "Engine: 89\n",
      "Engine: 90\n",
      "Engine: 91\n",
      "Engine: 92\n",
      "Engine: 93\n",
      "Engine: 94\n",
      "Engine: 95\n",
      "Engine: 96\n",
      "Engine: 97\n",
      "Engine: 98\n",
      "Engine: 99\n",
      "Engine: 0\n",
      "Engine: 1\n",
      "Engine: 2\n",
      "Engine: 3\n",
      "Engine: 4\n",
      "Engine: 5\n",
      "Engine: 6\n",
      "Engine: 7\n",
      "Engine: 8\n",
      "Engine: 9\n",
      "Engine: 10\n",
      "Engine: 11\n",
      "Engine: 12\n",
      "Engine: 13\n",
      "Engine: 14\n",
      "Engine: 15\n",
      "Engine: 16\n",
      "Engine: 17\n",
      "Engine: 18\n",
      "Engine: 19\n",
      "Engine: 20\n",
      "Engine: 21\n",
      "Engine: 22\n",
      "Engine: 23\n",
      "Engine: 24\n",
      "Engine: 25\n",
      "Engine: 26\n",
      "Engine: 27\n",
      "Engine: 28\n",
      "Engine: 29\n",
      "Engine: 30\n",
      "Engine: 31\n",
      "Engine: 32\n",
      "Engine: 33\n",
      "Engine: 34\n",
      "Engine: 35\n",
      "Engine: 36\n",
      "Engine: 37\n",
      "Engine: 38\n",
      "Engine: 39\n",
      "Engine: 40\n",
      "Engine: 41\n",
      "Engine: 42\n",
      "Engine: 43\n",
      "Engine: 44\n",
      "Engine: 45\n",
      "Engine: 46\n",
      "Engine: 47\n",
      "Engine: 48\n",
      "Engine: 49\n",
      "Engine: 50\n",
      "Engine: 51\n",
      "Engine: 52\n",
      "Engine: 53\n",
      "Engine: 54\n",
      "Engine: 55\n",
      "Engine: 56\n",
      "Engine: 57\n",
      "Engine: 58\n",
      "Engine: 59\n",
      "Engine: 60\n",
      "Engine: 61\n",
      "Engine: 62\n",
      "Engine: 63\n",
      "Engine: 64\n",
      "Engine: 65\n",
      "Engine: 66\n",
      "Engine: 67\n",
      "Engine: 68\n",
      "Engine: 69\n",
      "Engine: 70\n",
      "Engine: 71\n",
      "Engine: 72\n",
      "Engine: 73\n",
      "Engine: 74\n",
      "Engine: 75\n",
      "Engine: 76\n",
      "Engine: 77\n",
      "Engine: 78\n",
      "Engine: 79\n",
      "Engine: 80\n",
      "Engine: 81\n",
      "Engine: 82\n",
      "Engine: 83\n",
      "Engine: 84\n",
      "Engine: 85\n",
      "Engine: 86\n",
      "Engine: 87\n",
      "Engine: 88\n",
      "Engine: 89\n",
      "Engine: 90\n",
      "Engine: 91\n",
      "Engine: 92\n",
      "Engine: 93\n",
      "Engine: 94\n",
      "Engine: 95\n",
      "Engine: 96\n",
      "Engine: 97\n",
      "Engine: 98\n",
      "Engine: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\keras\\optimizers\\legacy\\rmsprop.py:143: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'nb_epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\UNI L4S1\\Final Year Project With Grace\\GRACE-Final_Model\\GRACE-RNN\\GRACE-WTTE-RNN\\WTTE-RNN\\keras-wtte-rnn-master\\GRACE_WTTE_RNN.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 151>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/UNI%20L4S1/Final%20Year%20Project%20With%20Grace/GRACE-Final_Model/GRACE-RNN/GRACE-WTTE-RNN/WTTE-RNN/keras-wtte-rnn-master/GRACE_WTTE_RNN.ipynb#W0sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39mweibull_loglik_discrete, optimizer\u001b[39m=\u001b[39mRMSprop(lr\u001b[39m=\u001b[39m\u001b[39m.001\u001b[39m))\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/UNI%20L4S1/Final%20Year%20Project%20With%20Grace/GRACE-Final_Model/GRACE-RNN/GRACE-WTTE-RNN/WTTE-RNN/keras-wtte-rnn-master/GRACE_WTTE_RNN.ipynb#W0sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m \u001b[39m# Fit!\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/UNI%20L4S1/Final%20Year%20Project%20With%20Grace/GRACE-Final_Model/GRACE-RNN/GRACE-WTTE-RNN/WTTE-RNN/keras-wtte-rnn-master/GRACE_WTTE_RNN.ipynb#W0sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_x, train_y, nb_epoch\u001b[39m=\u001b[39;49m\u001b[39m250\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m2000\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(test_x, test_y))\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/UNI%20L4S1/Final%20Year%20Project%20With%20Grace/GRACE-Final_Model/GRACE-RNN/GRACE-WTTE-RNN/WTTE-RNN/keras-wtte-rnn-master/GRACE_WTTE_RNN.ipynb#W0sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m \u001b[39m# Make some predictions and put them alongside the real TTE and event indicator values\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/UNI%20L4S1/Final%20Year%20Project%20With%20Grace/GRACE-Final_Model/GRACE-RNN/GRACE-WTTE-RNN/WTTE-RNN/keras-wtte-rnn-master/GRACE_WTTE_RNN.ipynb#W0sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m test_predict \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(test_x)\n",
      "File \u001b[1;32md:\\Users\\Arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\Users\\Arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "\u001b[1;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'nb_epoch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Masking\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as k\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\"\"\"\n",
    "    Discrete log-likelihood for Weibull hazard function on censored survival data\n",
    "    y_true is a (samples, 2) tensor containing time-to-event (y), and an event indicator (u)\n",
    "    ab_pred is a (samples, 2) tensor containing predicted Weibull alpha (a) and beta (b) parameters\n",
    "    For math, see https://ragulpr.github.io/assets/draft_master_thesis_martinsson_egil_wtte_rnn_2016.pdf (Page 35)\n",
    "\"\"\"\n",
    "def weibull_loglik_discrete(y_true, ab_pred, name=None):\n",
    "    y_ = y_true[:, 0]\n",
    "    u_ = y_true[:, 1]\n",
    "    a_ = ab_pred[:, 0]\n",
    "    b_ = ab_pred[:, 1]\n",
    "\n",
    "    hazard0 = k.pow((y_ + 1e-35) / a_, b_)\n",
    "    hazard1 = k.pow((y_ + 1) / a_, b_)\n",
    "\n",
    "    return -1 * k.mean(u_ * k.log(k.exp(hazard1 - hazard0) - 1.0) - hazard1)\n",
    "\n",
    "\"\"\"\n",
    "    Not used for this model, but included in case somebody needs it\n",
    "    For math, see https://ragulpr.github.io/assets/draft_master_thesis_martinsson_egil_wtte_rnn_2016.pdf (Page 35)\n",
    "\"\"\"\n",
    "def weibull_loglik_continuous(y_true, ab_pred, name=None):\n",
    "    y_ = y_true[:, 0]\n",
    "    u_ = y_true[:, 1]\n",
    "    a_ = ab_pred[:, 0]\n",
    "    b_ = ab_pred[:, 1]\n",
    "\n",
    "    ya = (y_ + 1e-35) / a_\n",
    "    return -1 * k.mean(u_ * (k.log(b_) + b_ * k.log(ya)) - k.pow(ya, b_))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Custom Keras activation function, outputs alpha neuron using exponentiation and beta using softplus\n",
    "\"\"\"\n",
    "def activate(ab):\n",
    "    a = k.exp(ab[:, 0])\n",
    "    b = k.softplus(ab[:, 1])\n",
    "\n",
    "    a = k.reshape(a, (k.shape(a)[0], 1))\n",
    "    b = k.reshape(b, (k.shape(b)[0], 1))\n",
    "\n",
    "    return k.concatenate((a, b), axis=1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Load and parse engine data files into:\n",
    "       - an (engine/day, observed history, sensor readings) x tensor, where observed history is 100 days, zero-padded\n",
    "         for days that don't have a full 100 days of observed history (e.g., first observed day for an engine)\n",
    "       - an (engine/day, 2) tensor containing time-to-event and 1 (since all engines failed)\n",
    "\n",
    "    There are probably MUCH better ways of doing this, but I don't use Numpy that much, and the data parsing isn't the\n",
    "    point of this demo anyway.\n",
    "\"\"\"\n",
    "def load_file(name):\n",
    "    with open(name, 'r') as file:\n",
    "        return np.loadtxt(file, delimiter=',')\n",
    "\n",
    "np.set_printoptions(suppress=True, threshold=10000)\n",
    "\n",
    "train = load_file('train.csv')\n",
    "test_x = load_file('test_x.csv')\n",
    "test_y = load_file('test_y.csv')\n",
    "\n",
    "# Combine the X values to normalize them, then split them back out\n",
    "all_x = np.concatenate((train[:, 2:26], test_x[:, 2:26]))\n",
    "all_x = normalize(all_x, axis=0)\n",
    "\n",
    "train[:, 2:26] = all_x[0:train.shape[0], :]\n",
    "test_x[:, 2:26] = all_x[train.shape[0]:, :]\n",
    "\n",
    "# Make engine numbers and days zero-indexed, for everybody's sanity\n",
    "train[:, 0:2] -= 1\n",
    "test_x[:, 0:2] -= 1\n",
    "\n",
    "# Configurable observation look-back period for each engine/day\n",
    "max_time = 100\n",
    "\n",
    "def build_data(engine, time, x, max_time, is_test):\n",
    "    # y[0] will be days remaining, y[1] will be event indicator, always 1 for this data\n",
    "    out_y = np.empty((0, 2), dtype=np.float32)\n",
    "\n",
    "    # A full history of sensor readings to date for each x\n",
    "    out_x = np.empty((0, max_time, 24), dtype=np.float32)\n",
    "\n",
    "    for i in range(100):\n",
    "        print(\"Engine: \" + str(i))\n",
    "        # When did the engine fail? (Last day + 1 for train data, irrelevant for test.)\n",
    "        max_engine_time = int(np.max(time[engine == i])) + 1\n",
    "\n",
    "        if is_test:\n",
    "            start = max_engine_time - 1\n",
    "        else:\n",
    "            start = 0\n",
    "\n",
    "        this_x = np.empty((0, max_time, 24), dtype=np.float32)\n",
    "\n",
    "        for j in range(start, max_engine_time):\n",
    "            engine_x = x[engine == i]\n",
    "\n",
    "            out_y = np.append(out_y, np.array((max_engine_time - j, 1), ndmin=2), axis=0)\n",
    "\n",
    "            xtemp = np.zeros((1, max_time, 24))\n",
    "            xtemp[:, max_time-min(j, 99)-1:max_time, :] = engine_x[max(0, j-max_time+1):j+1, :]\n",
    "            this_x = np.concatenate((this_x, xtemp))\n",
    "\n",
    "        out_x = np.concatenate((out_x, this_x))\n",
    "\n",
    "    return out_x, out_y\n",
    "\n",
    "train_x, train_y = build_data(train[:, 0], train[:, 1], train[:, 2:26], max_time, False)\n",
    "test_x = build_data(test_x[:, 0], test_x[:, 1], test_x[:, 2:26], max_time, True)[0]\n",
    "\n",
    "train_u = np.zeros((100, 1), dtype=np.float32)\n",
    "train_u += 1\n",
    "test_y = np.append(np.reshape(test_y, (100, 1)), train_u, axis=1)\n",
    "\n",
    "\"\"\"\n",
    "    Here's the rest of the meat of the demo... actually fitting and training the model.\n",
    "    We'll also make some test predictions so we can evaluate model performance.\n",
    "\"\"\"\n",
    "\n",
    "# Start building our model\n",
    "model = Sequential()\n",
    "\n",
    "# Mask parts of the lookback period that are all zeros (i.e., unobserved) so they don't skew the model\n",
    "model.add(Masking(mask_value=0., input_shape=(max_time, 24)))\n",
    "\n",
    "# LSTM is just a common type of RNN. You could also try anything else (e.g., GRU).\n",
    "model.add(LSTM(20, input_dim=24))\n",
    "\n",
    "# We need 2 neurons to output Alpha and Beta parameters for our Weibull distribution\n",
    "model.add(Dense(2))\n",
    "\n",
    "# Apply the custom activation function mentioned above\n",
    "model.add(Activation(activate))\n",
    "\n",
    "# Use the discrete log-likelihood for Weibull survival data as our loss function\n",
    "model.compile(loss=weibull_loglik_discrete, optimizer=RMSprop(lr=.001))\n",
    "\n",
    "# Fit!\n",
    "model.fit(train_x, train_y, nb_epoch=250, batch_size=2000, verbose=2, validation_data=(test_x, test_y))\n",
    "\n",
    "# Make some predictions and put them alongside the real TTE and event indicator values\n",
    "test_predict = model.predict(test_x)\n",
    "test_predict = np.resize(test_predict, (100, 2))\n",
    "test_result = np.concatenate((test_y, test_predict), axis=1)\n",
    "\n",
    "# TTE, Event Indicator, Alpha, Beta\n",
    "print(test_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "Over Number\t1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\t12\t13\t14\t15\t16\t17\t18\t19\t20\n",
    "TTE\t3\t2\t1\t1\t3\t2\t1\t1\t6\t5\t4\t3\t2\t1\t3\t2\t1\t0\t0\t0\n",
    "Uncencored \t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
