{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRAISE THE LORD ALMIGHTY FATHER Thank You JESUS Praise You JESUS Forever May the HOLY SPIRIT Guide Us MOTHER MARY PRAY For Us SAINT JOSEPH, PRAY For Us SAINT ANTHONY PRAY For Us GOD BLESS !!! â›ªðŸ§¡âœðŸ§¡â›ª\n"
     ]
    }
   ],
   "source": [
    "print(\"PRAISE THE LORD ALMIGHTY FATHER Thank You JESUS Praise You JESUS Forever May the HOLY SPIRIT Guide Us MOTHER MARY PRAY For Us SAINT JOSEPH, PRAY For Us SAINT ANTHONY PRAY For Us GOD BLESS !!! â›ªðŸ§¡âœðŸ§¡â›ª\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creted for Random Code Testing "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Motor features\n",
    "num_samples = 1000\n",
    "motor_vibration = np.random.normal(loc=0.0, scale=1.0, size=num_samples)\n",
    "motor_voltage = np.random.normal(loc=220.0, scale=10.0, size=num_samples)\n",
    "\n",
    "# Time-related columns\n",
    "start_date = datetime(2022, 1, 1)\n",
    "timestamps = [start_date + timedelta(days=i) for i in range(num_samples)]\n",
    "breakdown_time = np.random.randint(low=30, high=200, size=num_samples)  # Random breakdown time between 30 and 200 days\n",
    "breakdown_event = np.random.choice([0, 1], size=num_samples, p=[0.95, 0.05])  # Probability of breakdown event (0 or 1)\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'motor_vibration': motor_vibration,\n",
    "    'motor_voltage': motor_voltage,\n",
    "    'breakdown_time': breakdown_time,\n",
    "    'breakdown_event': breakdown_event\n",
    "})\n",
    "\n",
    "# Add additional columns if needed\n",
    "data['age_at_breakdown'] = (data['timestamp'] - start_date).dt.days\n",
    "# Add more columns as necessary\n",
    "\n",
    "# Save the data to a CSV file\n",
    "data.to_csv('motor_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Motor features\n",
    "num_samples = 1000\n",
    "motor_vibration = np.random.normal(loc=0.0, scale=1.0, size=num_samples)\n",
    "motor_voltage = np.random.normal(loc=220.0, scale=10.0, size=num_samples)\n",
    "\n",
    "# Time-related columns\n",
    "start_date = datetime(2022, 1, 1)\n",
    "timestamps = [start_date + timedelta(days=i) for i in range(num_samples)]\n",
    "breakdown_time = np.random.randint(low=30, high=200, size=num_samples)  # Random breakdown time between 30 and 200 days\n",
    "breakdown_event = np.random.choice([0, 1], size=num_samples, p=[0.8, 0.2])  # Probability of breakdown event (0 or 1)\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'motor_vibration': motor_vibration,\n",
    "    'motor_voltage': motor_voltage,\n",
    "    'breakdown_time': breakdown_time,\n",
    "    'breakdown_event': breakdown_event\n",
    "})\n",
    "\n",
    "# Add additional columns if needed\n",
    "data['age_at_breakdown'] = (data['timestamp'] - start_date).dt.days\n",
    "# Add more columns as necessary\n",
    "\n",
    "# Save the data to a CSV file\n",
    "data.to_csv('motor_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lifelines\n",
      "  Downloading lifelines-0.27.7-py3-none-any.whl (409 kB)\n",
      "     ------------------------------------ 409.4/409.4 kB 608.5 kB/s eta 0:00:00\n",
      "Collecting autograd-gamma>=0.3\n",
      "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting formulaic>=0.2.2\n",
      "  Downloading formulaic-0.6.1-py3-none-any.whl (82 kB)\n",
      "     ---------------------------------------- 82.3/82.3 kB 2.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.14.0 in d:\\users\\arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from lifelines) (1.23.1)\n",
      "Requirement already satisfied: matplotlib>=3.0 in d:\\users\\arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from lifelines) (3.7.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in d:\\users\\arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from lifelines) (1.10.1)\n",
      "Requirement already satisfied: pandas>=1.0.0 in d:\\users\\arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from lifelines) (1.4.3)\n",
      "Collecting autograd>=1.5\n",
      "  Downloading autograd-1.5-py3-none-any.whl (48 kB)\n",
      "     ---------------------------------------- 48.9/48.9 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting future>=0.15.2\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\n",
      "     -------------------------------------- 840.9/840.9 kB 1.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: wrapt>=1.0 in d:\\users\\arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from formulaic>=0.2.2->lifelines) (1.14.1)\n",
      "Collecting astor>=0.8\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting interface-meta>=1.2.0\n",
      "  Downloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
      "Collecting graphlib-backport>=1.0.0\n",
      "  Downloading graphlib_backport-1.0.3-py3-none-any.whl (5.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in d:\\users\\arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from formulaic>=0.2.2->lifelines) (4.6.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\users\\arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from matplotlib>=3.0->lifelines) (21.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\users\\arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from matplotlib>=3.0->lifelines) (1.0.7)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in d:\\users\\arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from matplotlib>=3.0->lifelines) (5.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\users\\arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from matplotlib>=3.0->lifelines) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\users\\arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from matplotlib>=3.0->lifelines) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\users\\arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from matplotlib>=3.0->lifelines) (9.5.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\users\\arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from matplotlib>=3.0->lifelines) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\users\\arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from matplotlib>=3.0->lifelines) (4.39.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\users\\arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from matplotlib>=3.0->lifelines) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\users\\arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from pandas>=1.0.0->lifelines) (2022.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in d:\\users\\arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=3.0->lifelines) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\users\\arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.16.0)\n",
      "Building wheels for collected packages: autograd-gamma, future\n",
      "  Building wheel for autograd-gamma (setup.py): started\n",
      "  Building wheel for autograd-gamma (setup.py): finished with status 'done'\n",
      "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4032 sha256=13e9a83e318623e314407bae9cfa5820c1b9f75dacca146744666b5b33f48642\n",
      "  Stored in directory: c:\\users\\arul\\appdata\\local\\pip\\cache\\wheels\\16\\a2\\b6\\582cfdfbeeccd469504a01af3bb952fd9e7eccba40995eafea\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492025 sha256=efe6d2d07d3710f0920ecb3ce3d3ac1a378972f8e1a4f38199116136e250ba97\n",
      "  Stored in directory: c:\\users\\arul\\appdata\\local\\pip\\cache\\wheels\\a0\\0b\\ee\\e6994fadb42c1354dcccb139b0bf2795271bddfe6253ccdf11\n",
      "Successfully built autograd-gamma future\n",
      "Installing collected packages: interface-meta, graphlib-backport, future, astor, autograd, formulaic, autograd-gamma, lifelines\n",
      "Successfully installed astor-0.8.1 autograd-1.5 autograd-gamma-0.5.0 formulaic-0.6.1 future-0.18.3 graphlib-backport-1.0.3 interface-meta-1.3.0 lifelines-0.27.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lifelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Motor features\n",
    "num_samples = 1000\n",
    "motor_vibration = np.random.normal(loc=0.0, scale=1.0, size=num_samples)\n",
    "motor_voltage = np.random.normal(loc=220.0, scale=10.0, size=num_samples)\n",
    "\n",
    "# Time-related columns\n",
    "start_date = datetime(2022, 1, 1)\n",
    "timestamps = [start_date + timedelta(days=i) for i in range(num_samples)]\n",
    "breakdown_event = np.random.choice([0, 1], size=num_samples, p=[0.95, 0.05])  # Probability of breakdown event (0 or 1)\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'motor_vibration': motor_vibration,\n",
    "    'motor_voltage': motor_voltage,\n",
    "    'breakdown_event': breakdown_event\n",
    "})\n",
    "\n",
    "# Sort the data by timestamp\n",
    "data.sort_values(by='timestamp', inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Add time to event column\n",
    "data['time_to_event'] = np.inf  # Initialize with infinity\n",
    "prev_failure_index = -1\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if row['breakdown_event'] == 1:\n",
    "        prev_failure_index = index\n",
    "    if prev_failure_index != -1:\n",
    "        data.at[index, 'time_to_event'] = index - prev_failure_index\n",
    "\n",
    "# Add additional columns if needed\n",
    "data['age_at_breakdown'] = (data['timestamp'] - start_date).dt.days\n",
    "# Add more columns as necessary\n",
    "\n",
    "\n",
    "# Save the data to a CSV file\n",
    "data.to_csv('motor_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'time_to_event'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\UNI L4S1\\Final Year Project With Grace\\GRACE-Final_Model\\GRACE-RNN\\GRACE_Testing.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/UNI%20L4S1/Final%20Year%20Project%20With%20Grace/GRACE-Final_Model/GRACE-RNN/GRACE_Testing.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Fit the Cox proportional hazards model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/UNI%20L4S1/Final%20Year%20Project%20With%20Grace/GRACE-Final_Model/GRACE-RNN/GRACE_Testing.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m cph \u001b[39m=\u001b[39m CoxPHFitter()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/UNI%20L4S1/Final%20Year%20Project%20With%20Grace/GRACE-Final_Model/GRACE-RNN/GRACE_Testing.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m cph\u001b[39m.\u001b[39;49mfit(pd\u001b[39m.\u001b[39;49mDataFrame(train_features_scaled, columns\u001b[39m=\u001b[39;49mtrain_features\u001b[39m.\u001b[39;49mcolumns),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/UNI%20L4S1/Final%20Year%20Project%20With%20Grace/GRACE-Final_Model/GRACE-RNN/GRACE_Testing.ipynb#X13sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         duration_col\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtime_to_event\u001b[39;49m\u001b[39m'\u001b[39;49m, event_col\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbreakdown_event\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/UNI%20L4S1/Final%20Year%20Project%20With%20Grace/GRACE-Final_Model/GRACE-RNN/GRACE_Testing.ipynb#X13sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Step 5: Model evaluation\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/UNI%20L4S1/Final%20Year%20Project%20With%20Grace/GRACE-Final_Model/GRACE-RNN/GRACE_Testing.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Evaluate the model's performance on the validation set\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/UNI%20L4S1/Final%20Year%20Project%20With%20Grace/GRACE-Final_Model/GRACE-RNN/GRACE_Testing.ipynb#X13sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m concordance_index \u001b[39m=\u001b[39m cph\u001b[39m.\u001b[39mscore(pd\u001b[39m.\u001b[39mDataFrame(val_features_scaled, columns\u001b[39m=\u001b[39mval_features\u001b[39m.\u001b[39mcolumns),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/UNI%20L4S1/Final%20Year%20Project%20With%20Grace/GRACE-Final_Model/GRACE-RNN/GRACE_Testing.ipynb#X13sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m                              duration_col\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbreakdown_time\u001b[39m\u001b[39m'\u001b[39m, event_col\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbreakdown_event\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Users\\Arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\lifelines\\utils\\__init__.py:56\u001b[0m, in \u001b[0;36mCensoringType.right_censoring.<locals>.f\u001b[1;34m(model, *args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39m@wraps\u001b[39m(function)\n\u001b[0;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     55\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mset_censoring_type(model, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mRIGHT)\n\u001b[1;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m function(model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Users\\Arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\lifelines\\fitters\\coxph_fitter.py:290\u001b[0m, in \u001b[0;36mCoxPHFitter.fit\u001b[1;34m(self, df, duration_col, event_col, show_progress, initial_point, strata, weights_col, cluster_col, robust, batch_mode, timeline, formula, entry_col, fit_options)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[39mFit the Cox proportional hazard model to a right-censored dataset. Alias of `fit_right_censoring`.\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    287\u001b[0m \n\u001b[0;32m    288\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrata \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39m_to_list_or_singleton(utils\u001b[39m.\u001b[39mcoalesce(strata, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrata))\n\u001b[1;32m--> 290\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_model(\n\u001b[0;32m    291\u001b[0m     df,\n\u001b[0;32m    292\u001b[0m     duration_col,\n\u001b[0;32m    293\u001b[0m     event_col\u001b[39m=\u001b[39;49mevent_col,\n\u001b[0;32m    294\u001b[0m     show_progress\u001b[39m=\u001b[39;49mshow_progress,\n\u001b[0;32m    295\u001b[0m     initial_point\u001b[39m=\u001b[39;49minitial_point,\n\u001b[0;32m    296\u001b[0m     strata\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrata,\n\u001b[0;32m    297\u001b[0m     weights_col\u001b[39m=\u001b[39;49mweights_col,\n\u001b[0;32m    298\u001b[0m     cluster_col\u001b[39m=\u001b[39;49mcluster_col,\n\u001b[0;32m    299\u001b[0m     robust\u001b[39m=\u001b[39;49mrobust,\n\u001b[0;32m    300\u001b[0m     batch_mode\u001b[39m=\u001b[39;49mbatch_mode,\n\u001b[0;32m    301\u001b[0m     timeline\u001b[39m=\u001b[39;49mtimeline,\n\u001b[0;32m    302\u001b[0m     formula\u001b[39m=\u001b[39;49mformula,\n\u001b[0;32m    303\u001b[0m     entry_col\u001b[39m=\u001b[39;49mentry_col,\n\u001b[0;32m    304\u001b[0m     fit_options\u001b[39m=\u001b[39;49mfit_options,\n\u001b[0;32m    305\u001b[0m )\n\u001b[0;32m    306\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32md:\\Users\\Arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\lifelines\\fitters\\coxph_fitter.py:610\u001b[0m, in \u001b[0;36mCoxPHFitter._fit_model\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit_model\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    609\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbaseline_estimation_method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbreslow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 610\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_model_breslow(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    611\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbaseline_estimation_method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mspline\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    612\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_model_spline(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Users\\Arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\lifelines\\fitters\\coxph_fitter.py:623\u001b[0m, in \u001b[0;36mCoxPHFitter._fit_model_breslow\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    619\u001b[0m model \u001b[39m=\u001b[39m SemiParametricPHFitter(\n\u001b[0;32m    620\u001b[0m     penalizer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpenalizer, l1_ratio\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml1_ratio, strata\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrata, alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha, label\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_label\n\u001b[0;32m    621\u001b[0m )\n\u001b[0;32m    622\u001b[0m \u001b[39mif\u001b[39;00m utils\u001b[39m.\u001b[39mCensoringType\u001b[39m.\u001b[39mis_right_censoring(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 623\u001b[0m     model\u001b[39m.\u001b[39;49mfit(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    624\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[0;32m    625\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Users\\Arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\lifelines\\utils\\__init__.py:56\u001b[0m, in \u001b[0;36mCensoringType.right_censoring.<locals>.f\u001b[1;34m(model, *args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39m@wraps\u001b[39m(function)\n\u001b[0;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     55\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mset_censoring_type(model, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mRIGHT)\n\u001b[1;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m function(model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Users\\Arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\lifelines\\fitters\\coxph_fitter.py:1229\u001b[0m, in \u001b[0;36mSemiParametricPHFitter.fit\u001b[1;34m(self, df, duration_col, event_col, show_progress, initial_point, strata, weights_col, cluster_col, robust, batch_mode, timeline, formula, entry_col, fit_options)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformula \u001b[39m=\u001b[39m formula\n\u001b[0;32m   1227\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentry_col \u001b[39m=\u001b[39m entry_col\n\u001b[1;32m-> 1229\u001b[0m X, T, E, weights, entries, original_index, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clusters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess_dataframe(df)\n\u001b[0;32m   1231\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdurations \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m   1232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevent_observed \u001b[39m=\u001b[39m E\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[1;32md:\\Users\\Arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\lifelines\\fitters\\coxph_fitter.py:1306\u001b[0m, in \u001b[0;36mSemiParametricPHFitter._preprocess_dataframe\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m   1304\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1305\u001b[0m     sort_by \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mduration_col, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevent_col] \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevent_col \u001b[39melse\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mduration_col]\n\u001b[1;32m-> 1306\u001b[0m     df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49msort_values(by\u001b[39m=\u001b[39;49msort_by)\n\u001b[0;32m   1307\u001b[0m     original_index \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m   1309\u001b[0m \u001b[39m# Extract time, event and metadata\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\Arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Users\\Arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\pandas\\core\\frame.py:6301\u001b[0m, in \u001b[0;36mDataFrame.sort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   6296\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   6297\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLength of ascending (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(ascending)\u001b[39m}\u001b[39;00m\u001b[39m) != length of by (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(by)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   6298\u001b[0m     )\n\u001b[0;32m   6299\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(by) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m-> 6301\u001b[0m     keys \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_label_or_level_values(x, axis\u001b[39m=\u001b[39maxis) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m by]\n\u001b[0;32m   6303\u001b[0m     \u001b[39m# need to rewrap columns in Series to apply key function\u001b[39;00m\n\u001b[0;32m   6304\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   6305\u001b[0m         \u001b[39m# error: List comprehension has incompatible type List[Series];\u001b[39;00m\n\u001b[0;32m   6306\u001b[0m         \u001b[39m# expected List[ndarray]\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\Arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\pandas\\core\\frame.py:6301\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   6296\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   6297\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLength of ascending (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(ascending)\u001b[39m}\u001b[39;00m\u001b[39m) != length of by (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(by)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   6298\u001b[0m     )\n\u001b[0;32m   6299\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(by) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m-> 6301\u001b[0m     keys \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_label_or_level_values(x, axis\u001b[39m=\u001b[39;49maxis) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m by]\n\u001b[0;32m   6303\u001b[0m     \u001b[39m# need to rewrap columns in Series to apply key function\u001b[39;00m\n\u001b[0;32m   6304\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   6305\u001b[0m         \u001b[39m# error: List comprehension has incompatible type List[Series];\u001b[39;00m\n\u001b[0;32m   6306\u001b[0m         \u001b[39m# expected List[ndarray]\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\Arul\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\pandas\\core\\generic.py:1840\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1838\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\u001b[39m.\u001b[39mget_level_values(key)\u001b[39m.\u001b[39m_values\n\u001b[0;32m   1839\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[0;32m   1842\u001b[0m \u001b[39m# Check for duplicates\u001b[39;00m\n\u001b[0;32m   1843\u001b[0m \u001b[39mif\u001b[39;00m values\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'time_to_event'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lifelines import CoxPHFitter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Load the data\n",
    "data = pd.read_csv('motor_data.csv')  # Replace 'motor_data.csv' with your dataset filename\n",
    "\n",
    "# Step 2: Split the data into features and target variables\n",
    "features = data[['motor_vibration', 'motor_voltage']]  # Replace with your desired feature columns\n",
    "target_duration = data['breakdown_time']  # Time to event (breakdown time)\n",
    "target_event = data['breakdown_event']  # Event indicator (0 for non-breakdown, 1 for breakdown)\n",
    "\n",
    "# Step 3: Split the data into training and validation sets\n",
    "train_features, val_features, train_duration, val_duration, train_event, val_event = train_test_split(\n",
    "    features, target_duration, target_event, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 4: Model fitting\n",
    "# Prepare the features and target variables\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "val_features_scaled = scaler.transform(val_features)\n",
    "\n",
    "# Fit the Cox proportional hazards model\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(pd.DataFrame(train_features_scaled, columns=train_features.columns),\n",
    "        duration_col='time_to_event', event_col='breakdown_event')\n",
    "\n",
    "# Step 5: Model evaluation\n",
    "# Evaluate the model's performance on the validation set\n",
    "concordance_index = cph.score(pd.DataFrame(val_features_scaled, columns=val_features.columns),\n",
    "                             duration_col='breakdown_time', event_col='breakdown_event')\n",
    "print(f\"Concordance Index: {concordance_index}\")\n",
    "\n",
    "# Step 6: Model prediction\n",
    "# You can use the model to predict survival probabilities, hazard ratios, or make individual predictions on new data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
